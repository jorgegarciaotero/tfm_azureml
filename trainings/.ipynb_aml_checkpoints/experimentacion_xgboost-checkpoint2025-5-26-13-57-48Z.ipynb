{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install adlfs\n",
        "!pip install keras-tuner --quiet\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting adlfs\n  Downloading adlfs-2024.12.0-py3-none-any.whl.metadata (7.7 kB)\nCollecting azure-core<2.0.0,>=1.28.0 (from adlfs)\n  Downloading azure_core-1.34.0-py3-none-any.whl.metadata (42 kB)\n\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting azure-datalake-store<0.1,>=0.0.53 (from adlfs)\n  Downloading azure_datalake_store-0.0.53-py2.py3-none-any.whl.metadata (19 kB)\nCollecting azure-identity (from adlfs)\n  Downloading azure_identity-1.23.0-py3-none-any.whl.metadata (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting azure-storage-blob>=12.17.0 (from adlfs)\n  Downloading azure_storage_blob-12.25.1-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: fsspec>=2023.12.0 in /usr/local/lib/python3.11/dist-packages (from adlfs) (2025.3.2)\nRequirement already satisfied: aiohttp>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from adlfs) (3.11.15)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.0->adlfs) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.0->adlfs) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.0->adlfs) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.0->adlfs) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.0->adlfs) (6.4.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.0->adlfs) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7.0->adlfs) (1.20.1)\nRequirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0,>=1.28.0->adlfs) (2.32.3)\nRequirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0,>=1.28.0->adlfs) (1.17.0)\nRequirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0,>=1.28.0->adlfs) (4.14.0)\nRequirement already satisfied: cffi in /usr/local/lib/python3.11/dist-packages (from azure-datalake-store<0.1,>=0.0.53->adlfs) (1.17.1)\nCollecting msal<2,>=1.16.0 (from azure-datalake-store<0.1,>=0.0.53->adlfs)\n  Downloading msal-1.32.3-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from azure-storage-blob>=12.17.0->adlfs) (43.0.3)\nCollecting isodate>=0.6.1 (from azure-storage-blob>=12.17.0->adlfs)\n  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\nCollecting msal-extensions>=1.2.0 (from azure-identity->adlfs)\n  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi->azure-datalake-store<0.1,>=0.0.53->adlfs) (2.22)\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2,>=1.16.0->azure-datalake-store<0.1,>=0.0.53->adlfs) (2.10.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->adlfs) (2025.6.15)\nDownloading adlfs-2024.12.0-py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading azure_core-1.34.0-py3-none-any.whl (207 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading azure_datalake_store-0.0.53-py2.py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading azure_storage_blob-12.25.1-py3-none-any.whl (406 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.0/407.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading azure_identity-1.23.0-py3-none-any.whl (186 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.1/186.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\nDownloading msal-1.32.3-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\nInstalling collected packages: isodate, azure-core, azure-storage-blob, msal, msal-extensions, azure-datalake-store, azure-identity, adlfs\nSuccessfully installed adlfs-2024.12.0 azure-core-1.34.0 azure-datalake-store-0.0.53 azure-identity-1.23.0 azure-storage-blob-12.25.1 isodate-0.7.2 msal-1.32.3 msal-extensions-1.3.1\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2V3F2vDUwi6",
        "outputId": "b04fd437-c991-41e5-cf4b-58c64f0a6738"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IMPORTS"
      ],
      "metadata": {
        "id": "1wS-JMZwcAv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "import random\n",
        "import plotly.express as px\n",
        "import joblib\n",
        "from datetime import timedelta\n",
        "from typing import List, Tuple, Dict,Optional\n",
        "\n",
        "# Azure\n",
        "from adlfs import AzureBlobFileSystem\n",
        "\n",
        "from typing import Tuple, List, Optional\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Models\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score, confusion_matrix\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from xgboost import XGBClassifier\n",
        "import joblib"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "LBwAr_3QsEcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FUNCTIONS"
      ],
      "metadata": {
        "id": "b1bBHtKRcHMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data_from_dl(account_name: str,container_name: str,relative_path: str,access_key: str)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads all Parquet files from an Azure Blob Storage path into a single DataFrame.\n",
        "    Args:\n",
        "        - account_name (str): Azure Storage account name.\n",
        "        - container_name (str): Name of the container.\n",
        "        - relative_path (str): Path inside the container to search for .parquet files.\n",
        "        - access_key (str): Storage account access key.\n",
        "    Returns:\n",
        "        - df (pd.DataFrame): Combined DataFrame from all found Parquet files.\n",
        "    Raises:\n",
        "        - ValueError: If no Parquet files are found in the path.\n",
        "    \"\"\"\n",
        "    abfs = AzureBlobFileSystem(account_name=account_name, account_key=access_key)\n",
        "\n",
        "\n",
        "    all_files = abfs.glob(f\"{container_name}/{relative_path}/*.parquet\")\n",
        "    print(f\"folder: {all_files}\")\n",
        "\n",
        "    if not all_files:\n",
        "        raise ValueError(\"Not found .parquet files\")\n",
        "\n",
        "    dfs = []\n",
        "    for f in all_files:\n",
        "        print(f\"Reading files: {f}\")\n",
        "        with abfs.open(f, \"rb\") as fp:\n",
        "            dfs.append(pd.read_parquet(fp))\n",
        "\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    print(df.head())\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "ElZKSy8AdhJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(\n",
        "    y_test: pd.Series,\n",
        "    y_pred: pd.Series,\n",
        "    y_prob: pd.Series\n",
        ") -> Tuple[float, float, float, np.ndarray, float, float]:\n",
        "    \"\"\"\n",
        "    Evaluates the classification model and plots metrics.\n",
        "\n",
        "    Args:\n",
        "        y_test (pd.Series): True target values.\n",
        "        y_pred (pd.Series): Predicted class values.\n",
        "        y_prob (pd.Series): Predicted probabilities for class 1.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing Accuracy, F1 Score, ROC AUC, Confusion Matrix, Precision, Recall.\n",
        "    \"\"\"\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc = roc_auc_score(y_test, y_prob)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {roc:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "    cax = ax.matshow(cm, cmap='Blues')\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticks([0, 1])\n",
        "    ax.set_yticks([0, 1])\n",
        "    ax.set_xticklabels(['No', 'Yes'])\n",
        "    ax.set_yticklabels(['No', 'Yes'])\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return acc, f1, roc, cm, precision, recall\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "mBcU6X6byprl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Cleans a DataFrame by dropping unnecessary columns and handling missing values.\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): Input DataFrame.\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: Cleaned DataFrame.\n",
        "  \"\"\"\n",
        "  columns_to_drop = [\n",
        "      'capital_gains',\n",
        "      'ret_next_3m', 'ret_next_6m', 'ret_next_1y',\n",
        "      'price_lead_3m', 'price_lead_6m', 'price_lead_1y',\n",
        "      'open_v', 'high', 'low', 'dividends', 'stock_splits',\n",
        "      'is_dividend_day', 'is_stock_split', 'gap_open', 'price_range',\n",
        "      'tr_1', 'tr_2', 'tr_3', 'sma_5', 'bollinger_upper',\n",
        "      'bollinger_lower', 'ema_12', 'macd_line'\n",
        "  ]\n",
        "\n",
        "\n",
        "  print(f\"Shape before: {df.shape}\")\n",
        "  df = df.drop(columns=columns_to_drop, errors='ignore').copy()\n",
        "  numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\", \"int32\"]).columns\n",
        "  imputer = SimpleImputer(strategy=\"mean\")\n",
        "  df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
        "  print(f\"Shape after: {df.shape}\")\n",
        "  return df;"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "K3119lEWi0ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(\n",
        "    df: pd.DataFrame,\n",
        "    targets: list\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares a DataFrame for LSTM modeling: imputes missing values and applies MinMax scaling.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input data with features + target + symbol + date.\n",
        "        targets (list): List of target column names.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Scaled dataframe with symbol and date preserved.\n",
        "        MinMaxScaler: The fitted scaler object.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Separar columnas a preservar\n",
        "    symbol_col = df_clean[\"symbol\"]\n",
        "    date_col = df_clean[\"date\"]\n",
        "\n",
        "    # Eliminar columnas no necesarias para el modelo\n",
        "    df_clean = df_clean.drop(columns=[\"symbol\", \"date\"], errors=\"ignore\")\n",
        "\n",
        "    # Codificación de categóricas si existieran (seguridad)\n",
        "    for col in df_clean.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
        "        df_clean[col] = LabelEncoder().fit_transform(df_clean[col].astype(str))\n",
        "\n",
        "    # Separar features y targets\n",
        "    feature_cols = [col for col in df_clean.columns if col not in targets]\n",
        "    X = df_clean[feature_cols]\n",
        "    y = df_clean[targets]\n",
        "\n",
        "    # Imputación + escalado\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=df_clean.index)\n",
        "    df_scaled = pd.concat([df_scaled, y], axis=1)\n",
        "\n",
        "    # Reincorporar columnas para split posterior\n",
        "    df_scaled[\"symbol\"] = symbol_col.values\n",
        "    df_scaled[\"date\"] = date_col.values\n",
        "\n",
        "    print(f\"df_scaled: {df_scaled.shape}\")\n",
        "    print(f\"columns: {df_scaled.columns}\")\n",
        "    return df_scaled, scaler\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "id": "IRpJGpH6BuAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_symbol (df, target_column, test_size=0.2):\n",
        "    train_rows, test_rows = [], []\n",
        "\n",
        "    for symbol in df[\"symbol\"].unique():\n",
        "        df_symbol = df[df[\"symbol\"] == symbol].copy()\n",
        "        df_symbol = df_symbol.sort_values(\"date\")\n",
        "\n",
        "        split_idx = int(len(df_symbol) * (1 - test_size))\n",
        "        train_rows.append(df_symbol.iloc[:split_idx])\n",
        "        test_rows.append(df_symbol.iloc[split_idx:])\n",
        "\n",
        "    df_train = pd.concat(train_rows)\n",
        "    df_test = pd.concat(test_rows)\n",
        "\n",
        "    X_train = df_train.drop(columns=[target_column])\n",
        "    y_train = df_train[target_column]\n",
        "\n",
        "    X_test = df_test.drop(columns=[target_column])\n",
        "    y_test = df_test[target_column]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "YB9yRKoV2qaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_and_split_sequences_by_symbol(\n",
        "    df: pd.DataFrame,\n",
        "    target_column: str,\n",
        "    sequence_length: int = 60,\n",
        "    test_size: float = 0.2\n",
        "):\n",
        "    \"\"\"\n",
        "    Builds sequential data for each symbol independently and performs temporal train-test split.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Preprocessed DataFrame with features and target.\n",
        "        target_column (str): Column name for binary target.\n",
        "        sequence_length (int): Time window for each sequence.\n",
        "        test_size (float): Proportion of test samples (applied per symbol).\n",
        "\n",
        "    Returns:\n",
        "        Tuple of numpy arrays: X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    X_train, y_train, X_test, y_test = [], [], [], []\n",
        "\n",
        "    symbols = df[\"symbol\"].unique()\n",
        "    for symbol in symbols:\n",
        "        df_symbol = df[df[\"symbol\"] == symbol].copy()\n",
        "        df_symbol = df_symbol.sort_values(\"date\")\n",
        "\n",
        "        if len(df_symbol) <= sequence_length:\n",
        "            continue  # skip if not enough data\n",
        "\n",
        "        df_symbol[target_column] = df_symbol[target_column].astype(int)\n",
        "        features = df_symbol.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "        target = df_symbol[target_column].values\n",
        "        #features.to_csv(f\"{symbol}.csv\",sep=\";\")\n",
        "        #sys.exit(0)\n",
        "\n",
        "        split_idx = int(len(features) * (1 - test_size))\n",
        "        for i in range(len(features) - sequence_length):\n",
        "            if i + sequence_length >= len(features):\n",
        "                continue  # avoid index error\n",
        "\n",
        "            X_seq = features.iloc[i:i + sequence_length].values.astype(np.float32)\n",
        "            y_val = target[i + sequence_length]\n",
        "\n",
        "            if i + sequence_length < split_idx:\n",
        "                X_train.append(X_seq)\n",
        "                y_train.append(y_val)\n",
        "            else:\n",
        "                X_test.append(X_seq)\n",
        "                y_test.append(y_val)\n",
        "\n",
        "    return (\n",
        "        np.array(X_train),\n",
        "        np.array(X_test),\n",
        "        np.array(y_train),\n",
        "        np.array(y_test)\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "id": "kGdW-kiuBqv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_base_lstm_classifier(\n",
        "    X_train,\n",
        "    X_test,\n",
        "    y_train,\n",
        "    y_test,\n",
        "    sequence_length=60,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    patience=4,\n",
        "    class_weight=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains a simple LSTM classifier with basic architecture and early stopping.\n",
        "\n",
        "    Args:\n",
        "        X_train (np.ndarray): Training sequences.\n",
        "        X_test (np.ndarray): Test sequences.\n",
        "        y_train (np.ndarray): Training labels.\n",
        "        y_test (np.ndarray): Test labels.\n",
        "        sequence_length (int): Length of each input sequence.\n",
        "        epochs (int): Maximum number of epochs.\n",
        "        batch_size (int): Batch size.\n",
        "        patience (int): Early stopping patience.\n",
        "        class_weight (dict, optional): Class weights to handle imbalance.\n",
        "\n",
        "    Returns:\n",
        "        model (Sequential): Trained Keras model.\n",
        "        X_test (np.ndarray): Test features.\n",
        "        y_test (np.ndarray): Test labels.\n",
        "        y_prob (np.ndarray): Predicted probabilities.\n",
        "        y_pred (np.ndarray): Predicted binary classes.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(32, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "        Dropout(0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss=BinaryCrossentropy(),\n",
        "        metrics=['accuracy', AUC(name='auc')]\n",
        "    )\n",
        "\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor='val_auc',\n",
        "        mode='max',\n",
        "        patience=patience,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1,\n",
        "        class_weight=class_weight\n",
        "    )\n",
        "\n",
        "    y_prob = model.predict(X_test).flatten()\n",
        "    y_pred = (y_prob > 0.5).astype(int)\n",
        "\n",
        "    return model, X_test, y_test, y_prob, y_pred\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "y2tsmIYGd5gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_improved_lstm_classifier(\n",
        "    X_train,\n",
        "    X_test,\n",
        "    y_train,\n",
        "    y_test,\n",
        "    sequence_length=120,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    patience=6,\n",
        "    learning_rate=1e-4,\n",
        "    threshold=0.5,\n",
        "    class_weight=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains a bidirectional LSTM classifier with dropout, batch normalization, and threshold tuning.\n",
        "\n",
        "    Args:\n",
        "        X_train, X_test: Input sequences\n",
        "        y_train, y_test: Labels\n",
        "        sequence_length (int): Number of timesteps per sequence\n",
        "        epochs (int): Max epochs\n",
        "        batch_size (int): Training batch size\n",
        "        patience (int): Early stopping patience\n",
        "        learning_rate (float): Optimizer learning rate\n",
        "        threshold (float): Threshold to binarize output\n",
        "        class_weight (dict): Optional class weights\n",
        "\n",
        "    Returns:\n",
        "        model: Trained Keras model\n",
        "        X_test, y_test: Evaluation inputs\n",
        "        y_prob: Predicted probabilities\n",
        "        y_pred: Binary predictions based on threshold\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Bidirectional(LSTM(32, return_sequences=False)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss=BinaryCrossentropy(),\n",
        "        metrics=['accuracy', AUC(name='auc'), Precision(name='precision'), Recall(name='recall')]\n",
        "    )\n",
        "\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor='val_auc',\n",
        "        mode='max',\n",
        "        patience=patience,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1,\n",
        "        class_weight=class_weight\n",
        "    )\n",
        "\n",
        "    y_prob = model.predict(X_test).flatten()\n",
        "    y_pred = (y_prob > threshold).astype(int)\n",
        "\n",
        "    return model, X_test, y_test, y_prob, y_pred\n"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "Yo2TiInkWseb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(\n",
        "    y_test: pd.Series,\n",
        "    y_pred: pd.Series,\n",
        "    y_prob: pd.Series\n",
        ") -> Tuple[float, float, float, np.ndarray, float, float]:\n",
        "    \"\"\"\n",
        "    Evaluates the classification model and plots metrics.\n",
        "\n",
        "    Args:\n",
        "        y_test (pd.Series): True target values.\n",
        "        y_pred (pd.Series): Predicted class values.\n",
        "        y_prob (pd.Series): Predicted probabilities for class 1.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing Accuracy, F1 Score, ROC AUC, Confusion Matrix, Precision, Recall.\n",
        "    \"\"\"\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc = roc_auc_score(y_test, y_prob)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {roc:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "    cax = ax.matshow(cm, cmap='Blues')\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticks([0, 1])\n",
        "    ax.set_yticks([0, 1])\n",
        "    ax.set_xticklabels(['No', 'Yes'])\n",
        "    ax.set_yticklabels(['No', 'Yes'])\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return acc, f1, roc, cm, precision, recall\n"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "id": "SevMDZIdViOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN**"
      ],
      "metadata": {
        "id": "XtLZcyyJBu-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"config.yaml\", \"r\") as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "account_name = config['storage']['storage_account_name']\n",
        "container_name = config['storage']['container_name']\n",
        "relative_path = config['storage']['relative_path']\n",
        "access_key = config['storage']['access_key']\n",
        "\n",
        "# Cargar el DataFrame desde Azure\n",
        "df_full = load_data_from_dl(account_name, container_name, relative_path, access_key)\n",
        "df_full = df_full.dropna(subset=['ret_next_3m', 'ret_next_6m', 'ret_next_1y'])\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "folder: ['smart-wallet-dl/smart_wallet/stock_data_parquet/part-00000-tid-6319656906602864551-70988dce-8c71-437f-9993-81d0d24559f8-53-1-c000.snappy.parquet']\nReading files: smart-wallet-dl/smart_wallet/stock_data_parquet/part-00000-tid-6319656906602864551-70988dce-8c71-437f-9993-81d0d24559f8-53-1-c000.snappy.parquet\n         date  symbol      open_v        high         low     close_v  \\\n0  2023-07-25  TEL.OL   99.110617   99.423126   98.128441   98.128441   \n1  2023-07-25    ACIW   23.270000   23.530001   23.129999   23.420000   \n2  2023-07-25     TER  114.730270  117.172184  114.730270  116.368141   \n3  2023-07-25    ADCT    1.320000    1.330000    1.218000    1.290000   \n4  2023-07-25     TEX   59.353587   59.490459   57.574251   57.769783   \n\n      volume  dividends  stock_splits  capital_gains  ...  candle_color  \\\n0  1170518.0        0.0           0.0            NaN  ...           red   \n1   301500.0        0.0           0.0            NaN  ...         green   \n2  1256700.0        0.0           0.0            NaN  ...         green   \n3   811000.0        0.0           0.0            NaN  ...           red   \n4  1135700.0        0.0           0.0            NaN  ...           red   \n\n   momentum_10    roc_10    var_95  price_lead_3m  price_lead_6m  \\\n0     3.928711  0.041706 -0.016775     107.443146     113.383850   \n1     0.799999  0.035367 -0.031123      20.730000      30.410000   \n2     7.752586  0.071376 -0.020835      90.344109     110.100044   \n3    -0.560000 -0.302703 -0.068702       0.703000       2.100000   \n4    -1.759781 -0.029561 -0.032505      46.451183      56.749165   \n\n   price_lead_1y  ret_next_3m  ret_next_6m  ret_next_1y  \n0     125.240135     0.094924     0.155464     0.276288  \n1      42.599998    -0.114859     0.298463     0.818958  \n2     123.952568    -0.223635    -0.053864     0.065176  \n3       3.820000    -0.455039     0.627907     1.961240  \n4      65.014771    -0.195926    -0.017667     0.125411  \n\n[5 rows x 53 columns]\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bI6SenQBzyw",
        "outputId": "d464a5c2-220b-4ec1-ffa6-d0714d3108d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_full.to_parquet(\"df_full_spark.parquet\")\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "id": "I_z-KeuoV2lG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df_full=pd.read_parquet(\"df_full_spark.parquet\")"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "id": "YFbUfj75avdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Addtargets\n",
        "df_full[\"target_3m\"] = (df_full[\"ret_next_3m\"] > 0.1).astype(int)\n",
        "df_full[\"target_6m\"] = (df_full[\"ret_next_6m\"] > 0.1).astype(int)\n",
        "df_full[\"target_1y\"] = (df_full[\"ret_next_1y\"] > 0.1).astype(int)\n",
        "print(f\" target_3m: {df_full['target_3m'].value_counts()}\")\n",
        "print(f\" target_6m: {df_full['target_6m'].value_counts()}\")\n",
        "print(f\" target_1y: {df_full['target_1y'].value_counts()}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " target_3m: target_3m\n0    7475299\n1    3371248\nName: count, dtype: int64\n target_6m: target_6m\n0    6463478\n1    4383069\nName: count, dtype: int64\n target_1y: target_1y\n0    5554342\n1    5292205\nName: count, dtype: int64\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "id": "GVfvcr8UTf6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ac369e-a8f2-404c-c533-8ad637eca277"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBOOST"
      ],
      "metadata": {
        "id": "ffK_g38HmK5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_with_symbol_date(df: pd.DataFrame, targets: list):\n",
        "    \"\"\"\n",
        "    Prepares a DataFrame for LSTM modeling: encodes 'symbol', extracts 'date' features,\n",
        "    imputes missing values, and applies MinMax scaling.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input data with features + target + symbol + date.\n",
        "        targets (list): List of target column names.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Scaled dataframe with 'symbol' and 'date' incorporated as features.\n",
        "        MinMaxScaler: The fitted scaler object.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Convert date and extract features\n",
        "    df_clean[\"date\"] = pd.to_datetime(df_clean[\"date\"], errors=\"coerce\")\n",
        "    df_clean[\"year\"] = df_clean[\"date\"].dt.year\n",
        "    df_clean[\"month\"] = df_clean[\"date\"].dt.month\n",
        "    df_clean[\"dayofweek\"] = df_clean[\"date\"].dt.dayofweek\n",
        "\n",
        "    date_col = df_clean[\"date\"]  # store datetime (not string)\n",
        "\n",
        "    # Encode symbol (remains numeric)\n",
        "    df_clean[\"symbol\"] = LabelEncoder().fit_transform(df_clean[\"symbol\"].astype(str))\n",
        "\n",
        "    # Drop original date before scaling\n",
        "    df_clean = df_clean.drop(columns=[\"date\"], errors=\"ignore\")\n",
        "\n",
        "    for col in df_clean.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
        "        df_clean[col] = LabelEncoder().fit_transform(df_clean[col].astype(str))\n",
        "\n",
        "    feature_cols = [col for col in df_clean.columns if col not in targets]\n",
        "    X = df_clean[feature_cols]\n",
        "    y = df_clean[targets]\n",
        "\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=df_clean.index)\n",
        "    df_scaled = pd.concat([df_scaled, y], axis=1)\n",
        "\n",
        "    # Reattach encoded symbol and original date\n",
        "    df_scaled[\"symbol\"] = df_clean[\"symbol\"].values\n",
        "    df_scaled[\"date\"] = date_col.values\n",
        "\n",
        "    print(f\"columnas scaler: {X.columns}\")\n",
        "    return df_scaled, scaler\n"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "id": "bkwWVsPh21Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear targets binarios\n",
        "df_full[\"target_6m\"] = (df_full[\"ret_next_6m\"] > 0.05).astype(int)\n",
        "\n",
        "# Filtros\n",
        "top_symbols = df_full[\"symbol\"].unique()\n",
        "df_clean = df_full[df_full[\"symbol\"].isin(top_symbols)].copy()\n",
        "df_clean = df_clean.sort_values(by=[\"symbol\", \"date\"])\n",
        "\n",
        "# Limpieza e imputación\n",
        "df_clean = clean_columns(df_clean)\n",
        "\n",
        "# Preprocesamiento general\n",
        "targets = [\"target_6m\"]  # o el que quieras\n",
        "df_processed_minmax, _ = prepare_data_with_symbol_date(df_clean, targets=targets)\n",
        "print(f\"tipos: {df_processed_minmax.dtypes}\")\n",
        "\n",
        "# Dataset final\n",
        "X = df_processed_minmax.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "y = df_processed_minmax[\"target_6m\"]\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Pesos balanceados\n",
        "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "weight_dict = {i: w for i, w in enumerate(weights)}\n",
        "\n",
        "# Entrenamiento\n",
        "model = XGBClassifier(n_estimators=200, max_depth=4, learning_rate=0.05, scale_pos_weight=weight_dict[0]/weight_dict[1])\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicción\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluación\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Shape before: (10846547, 56)\nShape after: (10846547, 32)\ncolumnas scaler: Index(['symbol', 'close_v', 'volume', 'prev_close', 'prev_volume',\n       'daily_return', 'close_change_pct', 'intraday_volatility', 'log_return',\n       'volume_change_pct', 'sma_20', 'delta', 'gain', 'loss', 'rsi_14',\n       'rel_volume', 'ema_26', 'macd_signal', 'macd_histogram', 'true_range',\n       'atr_14', 'candle_body', 'upper_wick', 'lower_wick', 'candle_color',\n       'momentum_10', 'roc_10', 'var_95', 'target_3m', 'target_1y', 'year',\n       'month', 'dayofweek'],\n      dtype='object')\ntipos: symbol                          int64\nclose_v                       float64\nvolume                        float64\nprev_close                    float64\nprev_volume                   float64\ndaily_return                  float64\nclose_change_pct              float64\nintraday_volatility           float64\nlog_return                    float64\nvolume_change_pct             float64\nsma_20                        float64\ndelta                         float64\ngain                          float64\nloss                          float64\nrsi_14                        float64\nrel_volume                    float64\nema_26                        float64\nmacd_signal                   float64\nmacd_histogram                float64\ntrue_range                    float64\natr_14                        float64\ncandle_body                   float64\nupper_wick                    float64\nlower_wick                    float64\ncandle_color                  float64\nmomentum_10                   float64\nroc_10                        float64\nvar_95                        float64\ntarget_3m                     float64\ntarget_1y                     float64\nyear                          float64\nmonth                         float64\ndayofweek                     float64\ntarget_6m                     float64\ndate                   datetime64[ns]\ndtype: object\n              precision    recall  f1-score   support\n\n         0.0       0.65      0.66      0.65   1094399\n         1.0       0.65      0.63      0.64   1074911\n\n    accuracy                           0.65   2169310\n   macro avg       0.65      0.65      0.65   2169310\nweighted avg       0.65      0.65      0.65   2169310\n\nROC AUC: 0.7046227985747947\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_rxO98booBX",
        "outputId": "da965dcc-64a4-4d1f-ea09-ae5ca0dc1e25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear targets binarios\n",
        "df_full[\"target_6m\"] = (df_full[\"ret_next_6m\"] > 0.05).astype(int)\n",
        "\n",
        "# Filtros y orden\n",
        "top_symbols = df_full[\"symbol\"].unique()\n",
        "df_clean = df_full[df_full[\"symbol\"].isin(top_symbols)].copy()\n",
        "df_clean = df_clean.sort_values(by=[\"symbol\", \"date\"])\n",
        "\n",
        "# Limpieza e imputación\n",
        "df_clean = clean_columns(df_clean)\n",
        "\n",
        "# Preprocesamiento\n",
        "targets = [\"target_6m\"]\n",
        "df_processed_minmax, _ = prepare_data_with_symbol_date(df_clean, targets=targets)\n",
        "\n",
        "# Definir X e y\n",
        "X = df_processed_minmax.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "y = df_processed_minmax[\"target_6m\"]\n",
        "\n",
        "# División temporal\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Cálculo de pesos por clase\n",
        "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "weight_dict = {i: w for i, w in enumerate(weights)}\n",
        "\n",
        "# Entrenamiento con parámetros ya ajustados\n",
        "model = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=3,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=weight_dict[0] / weight_dict[1],\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicción\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluación\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Shape before: (10846547, 54)\nShape after: (10846547, 30)\ncolumnas scaler: Index(['symbol', 'close_v', 'volume', 'prev_close', 'prev_volume',\n       'daily_return', 'close_change_pct', 'intraday_volatility', 'log_return',\n       'volume_change_pct', 'sma_20', 'delta', 'gain', 'loss', 'rsi_14',\n       'rel_volume', 'ema_26', 'macd_signal', 'macd_histogram', 'true_range',\n       'atr_14', 'candle_body', 'upper_wick', 'lower_wick', 'candle_color',\n       'momentum_10', 'roc_10', 'var_95', 'year', 'month', 'dayofweek'],\n      dtype='object')\n              precision    recall  f1-score   support\n\n         0.0       0.64      0.64      0.64   1094399\n         1.0       0.63      0.63      0.63   1074911\n\n    accuracy                           0.64   2169310\n   macro avg       0.64      0.64      0.64   2169310\nweighted avg       0.64      0.64      0.64   2169310\n\nROC AUC: 0.6836790099500576\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNpe3MHRxGPi",
        "outputId": "af532e56-fc9c-47f0-f964-6539632ee0df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"XGBoost version:\", xgb.__version__)\n",
        "\n",
        "# Crear targets binarios con umbral del 10%\n",
        "df_full[\"target_6m\"] = (df_full[\"ret_next_6m\"] > 0.10).astype(int)\n",
        "\n",
        "# Usar todos los símbolos\n",
        "top_symbols = df_full[\"symbol\"].unique()\n",
        "df_clean = df_full[df_full[\"symbol\"].isin(top_symbols)].copy()\n",
        "df_clean = df_clean.sort_values(by=[\"symbol\", \"date\"])\n",
        "\n",
        "# Limpieza e imputación\n",
        "df_clean = clean_columns(df_clean)\n",
        "\n",
        "# Preprocesamiento\n",
        "targets = [\"target_6m\"]\n",
        "df_processed_minmax, _ = prepare_data_with_symbol_date(df_clean, targets=targets)\n",
        "\n",
        "# Crear feature de interacción\n",
        "df_processed_minmax[\"rsi_x_momentum\"] = df_processed_minmax[\"rsi_14\"] * df_processed_minmax[\"momentum_10\"]\n",
        "\n",
        "# Definir X e y\n",
        "X = df_processed_minmax.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "y = df_processed_minmax[\"target_6m\"]\n",
        "\n",
        "# Eliminar columnas con varianza casi nula\n",
        "selector = VarianceThreshold(threshold=1e-4)\n",
        "X = pd.DataFrame(selector.fit_transform(X), index=X.index)\n",
        "\n",
        "# Split temporal\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Pesos balanceados\n",
        "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "weight_dict = {i: w for i, w in enumerate(weights)}\n",
        "scale_weight = weight_dict[0] / weight_dict[1]\n",
        "\n",
        "# Param grid para GridSearchCV (27 combinaciones)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200, 300, 400],\n",
        "    \"max_depth\": [3, 4, 5],\n",
        "    \"learning_rate\": [0.01, 0.02, 0.03],\n",
        "    \"subsample\": [0.8],\n",
        "    \"colsample_bytree\": [0.8],\n",
        "    \"scale_pos_weight\": [scale_weight]\n",
        "}\n",
        "\n",
        "# Modelo base\n",
        "model = XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Grid Search optimizando F1\n",
        "grid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Entrenamiento final\n",
        "best_model = grid.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicción con threshold ajustado\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_prob > 0.4).astype(int)\n",
        "\n",
        "# Evaluación\n",
        "print(\"Mejores parámetros:\", grid.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "XGBoost version: 2.1.4\nShape before: (10846547, 56)\nShape after: (10846547, 32)\ncolumnas scaler: Index(['symbol', 'close_v', 'volume', 'prev_close', 'prev_volume',\n       'daily_return', 'close_change_pct', 'intraday_volatility', 'log_return',\n       'volume_change_pct', 'sma_20', 'delta', 'gain', 'loss', 'rsi_14',\n       'rel_volume', 'ema_26', 'macd_signal', 'macd_histogram', 'true_range',\n       'atr_14', 'candle_body', 'upper_wick', 'lower_wick', 'candle_color',\n       'momentum_10', 'roc_10', 'var_95', 'target_3m', 'target_1y', 'year',\n       'month', 'dayofweek'],\n      dtype='object')\nFitting 3 folds for each of 27 candidates, totalling 81 fits\nMejores parámetros: {'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 5, 'n_estimators': 400, 'scale_pos_weight': np.float64(0.6791664425130032), 'subsample': 0.8}\n              precision    recall  f1-score   support\n\n         0.0       0.68      0.82      0.75   1295892\n         1.0       0.62      0.43      0.51    873418\n\n    accuracy                           0.67   2169310\n   macro avg       0.65      0.63      0.63   2169310\nweighted avg       0.66      0.67      0.65   2169310\n\nROC AUC: 0.7020004222474785\nF1 Score: 0.5103308064384047\nConfusion Matrix:\n[[1068764  227128]\n [ 496393  377025]]\nTN=1068764, FP=227128, FN=496393, TP=377025\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV1PKM37Q6XB",
        "outputId": "52cc0a35-3bc9-4119-d883-991abd849b29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Crear targets binarios con umbral del 10%\n",
        "df_full[\"target_6m\"] = (df_full[\"ret_next_6m\"] > 0.10).astype(int)\n",
        "\n",
        "# Usar todos los símbolos\n",
        "top_symbols = df_full[\"symbol\"].unique()\n",
        "df_clean = df_full[df_full[\"symbol\"].isin(top_symbols)].copy()\n",
        "df_clean = df_clean.sort_values(by=[\"symbol\", \"date\"])\n",
        "\n",
        "# Limpieza e imputación\n",
        "df_clean = clean_columns(df_clean)\n",
        "\n",
        "# Preprocesamiento\n",
        "targets = [\"target_6m\"]\n",
        "df_processed_minmax, _ = prepare_data_with_symbol_date(df_clean, targets=targets)\n",
        "\n",
        "# Definir X e y\n",
        "X = df_processed_minmax.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "y = df_processed_minmax[\"target_6m\"]\n",
        "\n",
        "# Eliminar columnas con varianza casi nula\n",
        "selector = VarianceThreshold(threshold=1e-4)\n",
        "X = pd.DataFrame(selector.fit_transform(X), index=X.index)\n",
        "\n",
        "# Split temporal\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Pesos balanceados\n",
        "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "weight_dict = {i: w for i, w in enumerate(weights)}\n",
        "scale_weight = weight_dict[0] / weight_dict[1]\n",
        "\n",
        "# Param grid para GridSearchCV (27 combinaciones)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200, 300, 400],\n",
        "    \"max_depth\": [3, 4, 5],\n",
        "    \"learning_rate\": [0.01, 0.02, 0.03],\n",
        "    \"subsample\": [0.8],\n",
        "    \"colsample_bytree\": [0.8],\n",
        "    \"scale_pos_weight\": [scale_weight]\n",
        "}\n",
        "\n",
        "# Modelo base\n",
        "model = XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Grid Search optimizando F1\n",
        "grid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Entrenamiento final\n",
        "best_model = grid.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicción con threshold ajustado\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_prob > 0.4).astype(int)\n",
        "\n",
        "# Evaluación\n",
        "print(\"Mejores parámetros:\", grid.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "XGBoost version: 2.1.4\nShape before: (10846547, 56)\nShape after: (10846547, 32)\ncolumnas scaler: Index(['symbol', 'close_v', 'volume', 'prev_close', 'prev_volume',\n       'daily_return', 'close_change_pct', 'intraday_volatility', 'log_return',\n       'volume_change_pct', 'sma_20', 'delta', 'gain', 'loss', 'rsi_14',\n       'rel_volume', 'ema_26', 'macd_signal', 'macd_histogram', 'true_range',\n       'atr_14', 'candle_body', 'upper_wick', 'lower_wick', 'candle_color',\n       'momentum_10', 'roc_10', 'var_95', 'target_3m', 'target_1y', 'year',\n       'month', 'dayofweek'],\n      dtype='object')\nFitting 3 folds for each of 27 candidates, totalling 81 fits\nMejores parámetros: {'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 5, 'n_estimators': 400, 'scale_pos_weight': np.float64(0.6791664425130032), 'subsample': 0.8}\n              precision    recall  f1-score   support\n\n         0.0       0.68      0.83      0.75   1295892\n         1.0       0.63      0.42      0.50    873418\n\n    accuracy                           0.67   2169310\n   macro avg       0.65      0.63      0.63   2169310\nweighted avg       0.66      0.67      0.65   2169310\n\nROC AUC: 0.7020825798915501\nF1 Score: 0.5047045686596572\nConfusion Matrix:\n[[1074593  221299]\n [ 503920  369498]]\nTN=1074593, FP=221299, FN=503920, TP=369498\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbUz7BucROw7",
        "outputId": "a14de237-2e08-43ac-8d9b-4c4139515317"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"XGBoost version:\", xgb.__version__)\n",
        "\n",
        "# Crear targets binarios con umbral del 7%\n",
        "df_full[\"target_6m\"] = (df_full[\"ret_next_6m\"] > 0.07).astype(int)\n",
        "\n",
        "# Usar todos los símbolos\n",
        "top_symbols = df_full[\"symbol\"].unique()\n",
        "df_clean = df_full[df_full[\"symbol\"].isin(top_symbols)].copy()\n",
        "df_clean = df_clean.sort_values(by=[\"symbol\", \"date\"])\n",
        "\n",
        "# Limpieza e imputación\n",
        "df_clean = clean_columns(df_clean)\n",
        "\n",
        "# Preprocesamiento\n",
        "targets = [\"target_6m\"]\n",
        "df_processed_minmax, _ = prepare_data_with_symbol_date(df_clean, targets=targets)\n",
        "\n",
        "# Definir X e y\n",
        "X = df_processed_minmax.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "y = df_processed_minmax[\"target_6m\"]\n",
        "\n",
        "# Eliminar columnas con varianza casi nula\n",
        "selector = VarianceThreshold(threshold=1e-4)\n",
        "X = pd.DataFrame(selector.fit_transform(X), index=X.index)\n",
        "\n",
        "# Split temporal\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Pesos balanceados\n",
        "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "weight_dict = {i: w for i, w in enumerate(weights)}\n",
        "scale_weight = weight_dict[0] / weight_dict[1]\n",
        "\n",
        "# Param grid para GridSearchCV (27 combinaciones)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200, 300, 400],\n",
        "    \"max_depth\": [3, 4, 5],\n",
        "    \"learning_rate\": [0.01, 0.02, 0.03],\n",
        "    \"subsample\": [0.8],\n",
        "    \"colsample_bytree\": [0.8],\n",
        "    \"scale_pos_weight\": [scale_weight]\n",
        "}\n",
        "\n",
        "# Modelo base\n",
        "model = XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Grid Search optimizando F1\n",
        "grid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Entrenamiento final\n",
        "best_model = grid.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicción con threshold ajustado\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_prob > 0.4).astype(int)\n",
        "\n",
        "# Evaluación\n",
        "print(\"Mejores parámetros:\", grid.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "XGBoost version: 2.1.4\nShape before: (10846547, 56)\nShape after: (10846547, 32)\ncolumnas scaler: Index(['symbol', 'close_v', 'volume', 'prev_close', 'prev_volume',\n       'daily_return', 'close_change_pct', 'intraday_volatility', 'log_return',\n       'volume_change_pct', 'sma_20', 'delta', 'gain', 'loss', 'rsi_14',\n       'rel_volume', 'ema_26', 'macd_signal', 'macd_histogram', 'true_range',\n       'atr_14', 'candle_body', 'upper_wick', 'lower_wick', 'candle_color',\n       'momentum_10', 'roc_10', 'var_95', 'target_3m', 'target_1y', 'year',\n       'month', 'dayofweek'],\n      dtype='object')\nFitting 3 folds for each of 27 candidates, totalling 81 fits\nMejores parámetros: {'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 5, 'n_estimators': 400, 'scale_pos_weight': np.float64(0.8483387558593741), 'subsample': 0.8}\n              precision    recall  f1-score   support\n\n         0.0       0.70      0.58      0.64   1176642\n         1.0       0.59      0.70      0.64    992668\n\n    accuracy                           0.64   2169310\n   macro avg       0.64      0.64      0.64   2169310\nweighted avg       0.65      0.64      0.64   2169310\n\nROC AUC: 0.7035602680707781\nF1 Score: 0.637093633633827\nConfusion Matrix:\n[[687165 489477]\n [299836 692832]]\nTN=687165, FP=489477, FN=299836, TP=692832\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeUdpHaKWOKV",
        "outputId": "880d3a16-99ec-4b1a-80e7-c7ae0edce4b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Crear targets binarios con umbral del 5%\n",
        "df_full[\"target_6m\"] = (df_full[\"ret_next_6m\"] > 0.05).astype(int)\n",
        "\n",
        "# Usar todos los símbolos\n",
        "top_symbols = df_full[\"symbol\"].unique()\n",
        "df_clean = df_full[df_full[\"symbol\"].isin(top_symbols)].copy()\n",
        "df_clean = df_clean.sort_values(by=[\"symbol\", \"date\"])\n",
        "\n",
        "# Limpieza e imputación\n",
        "df_clean = clean_columns(df_clean)\n",
        "\n",
        "# Preprocesamiento\n",
        "targets = [\"target_6m\"]\n",
        "df_processed_minmax, _ = prepare_data_with_symbol_date(df_clean, targets=targets)\n",
        "\n",
        "# Definir X e y\n",
        "X = df_processed_minmax.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "y = df_processed_minmax[\"target_6m\"]\n",
        "\n",
        "# Eliminar columnas con varianza casi nula\n",
        "selector = VarianceThreshold(threshold=1e-4)\n",
        "X = pd.DataFrame(selector.fit_transform(X), index=X.index)\n",
        "\n",
        "# Split temporal\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Pesos balanceados\n",
        "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "weight_dict = {i: w for i, w in enumerate(weights)}\n",
        "scale_weight = weight_dict[0] / weight_dict[1]\n",
        "\n",
        "# Param grid para GridSearchCV (27 combinaciones)\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200, 300, 400],\n",
        "    \"max_depth\": [3, 4, 5],\n",
        "    \"learning_rate\": [0.01, 0.02, 0.03],\n",
        "    \"subsample\": [0.8],\n",
        "    \"colsample_bytree\": [0.8],\n",
        "    \"scale_pos_weight\": [scale_weight]\n",
        "}\n",
        "\n",
        "# Modelo base\n",
        "model = XGBClassifier(\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Grid Search optimizando F1\n",
        "grid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1\",\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Entrenamiento final\n",
        "best_model = grid.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicción con threshold ajustado\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_prob > 0.4).astype(int)\n",
        "\n",
        "# Evaluación\n",
        "print(\"Mejores parámetros:\", grid.best_params_)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "XGBoost version: 2.1.4\nShape before: (10846547, 56)\nShape after: (10846547, 32)\ncolumnas scaler: Index(['symbol', 'close_v', 'volume', 'prev_close', 'prev_volume',\n       'daily_return', 'close_change_pct', 'intraday_volatility', 'log_return',\n       'volume_change_pct', 'sma_20', 'delta', 'gain', 'loss', 'rsi_14',\n       'rel_volume', 'ema_26', 'macd_signal', 'macd_histogram', 'true_range',\n       'atr_14', 'candle_body', 'upper_wick', 'lower_wick', 'candle_color',\n       'momentum_10', 'roc_10', 'var_95', 'target_3m', 'target_1y', 'year',\n       'month', 'dayofweek'],\n      dtype='object')\nFitting 3 folds for each of 27 candidates, totalling 81 fits\nMejores parámetros: {'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 5, 'n_estimators': 400, 'scale_pos_weight': np.float64(0.9854333077829515), 'subsample': 0.8}\n              precision    recall  f1-score   support\n\n         0.0       0.73      0.41      0.53   1094399\n         1.0       0.59      0.85      0.69   1074911\n\n    accuracy                           0.63   2169310\n   macro avg       0.66      0.63      0.61   2169310\nweighted avg       0.66      0.63      0.61   2169310\n\nROC AUC: 0.7081325505082107\nF1 Score: 0.6929270715231209\nConfusion Matrix:\n[[448689 645710]\n [162747 912164]]\nTN=448689, FP=645710, FN=162747, TP=912164\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "id": "PrXDlNJediAe",
        "outputId": "1a39b0e8-6887-4e03-8d6d-54a1dc8c06f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score, confusion_matrix\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "\n",
        "print(\"XGBoost version:\", xgb.__version__)\n",
        "\n",
        "# Crear target binario con umbral del 5%\n",
        "df_full[\"target_6m\"] = (df_full[\"ret_next_6m\"] > 0.05).astype(int)\n",
        "\n",
        "# Ordenar y filtrar\n",
        "top_symbols = df_full[\"symbol\"].unique()\n",
        "df_clean = df_full[df_full[\"symbol\"].isin(top_symbols)].copy()\n",
        "df_clean = df_clean.sort_values(by=[\"symbol\", \"date\"])\n",
        "\n",
        "# Limpieza e imputación\n",
        "df_clean = clean_columns(df_clean)\n",
        "\n",
        "# Preprocesamiento\n",
        "targets = [\"target_6m\"]\n",
        "df_processed_minmax, _ = prepare_data_with_symbol_date(df_clean, targets=targets)\n",
        "\n",
        "# Feature extra\n",
        "df_processed_minmax[\"rsi_x_momentum\"] = df_processed_minmax[\"rsi_14\"] * df_processed_minmax[\"momentum_10\"]\n",
        "\n",
        "# Definir X e y\n",
        "X = df_processed_minmax.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "y = df_processed_minmax[\"target_6m\"]\n",
        "\n",
        "# VarianceThreshold\n",
        "selector = VarianceThreshold(threshold=1e-4)\n",
        "X_sel = pd.DataFrame(selector.fit_transform(X), index=X.index)\n",
        "features_selected = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "# División temporal\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Balanceo\n",
        "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "scale_weight = 0.9854333077829515  # hiperparámetro óptimo encontrado\n",
        "\n",
        "# Entrenamiento con hiperparámetros óptimos\n",
        "model = XGBClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=scale_weight,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicción\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_prob > 0.4).astype(int)\n",
        "\n",
        "# Evaluación\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "# Guardar artefactos\n",
        "joblib.dump(model, \"model_xgb_6m.pkl\")\n",
        "joblib.dump(selector, \"selector_xgb_6m.pkl\")\n",
        "joblib.dump(features_selected, \"features_xgb_6m.pkl\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "XGBoost version: 2.1.4\nShape before: (10846547, 56)\nShape after: (10846547, 32)\ncolumnas scaler: Index(['symbol', 'close_v', 'volume', 'prev_close', 'prev_volume',\n       'daily_return', 'close_change_pct', 'intraday_volatility', 'log_return',\n       'volume_change_pct', 'sma_20', 'delta', 'gain', 'loss', 'rsi_14',\n       'rel_volume', 'ema_26', 'macd_signal', 'macd_histogram', 'true_range',\n       'atr_14', 'candle_body', 'upper_wick', 'lower_wick', 'candle_color',\n       'momentum_10', 'roc_10', 'var_95', 'target_3m', 'target_1y', 'year',\n       'month', 'dayofweek'],\n      dtype='object')\n              precision    recall  f1-score   support\n\n         0.0       0.73      0.41      0.52   1094399\n         1.0       0.58      0.85      0.69   1074911\n\n    accuracy                           0.63   2169310\n   macro avg       0.66      0.63      0.61   2169310\nweighted avg       0.66      0.63      0.61   2169310\n\nROC AUC: 0.7067042595732963\nF1 Score: 0.6927564358869502\nConfusion Matrix:\n[[446312 648087]\n [161831 913080]]\nTN=446312, FP=648087, FN=161831, TP=913080\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "['features_xgb_6m.pkl']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "id": "nKD7teqweyL7",
        "outputId": "ac8b91c4-22a5-488d-e475-4ce80713c104",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score, confusion_matrix\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "\n",
        "print(\"XGBoost version:\", xgb.__version__)\n",
        "\n",
        "# Crear target binario con umbral del 5%\n",
        "df_full[\"target_3m\"] = (df_full[\"ret_next_3m\"] > 0.05).astype(int)\n",
        "\n",
        "# Ordenar y filtrar\n",
        "top_symbols = df_full[\"symbol\"].unique()\n",
        "df_clean = df_full[df_full[\"symbol\"].isin(top_symbols)].copy()\n",
        "df_clean = df_clean.sort_values(by=[\"symbol\", \"date\"])\n",
        "\n",
        "# Limpieza e imputación\n",
        "df_clean = clean_columns(df_clean)\n",
        "\n",
        "# Preprocesamiento\n",
        "targets = [\"target_3m\"]\n",
        "df_processed_minmax, _ = prepare_data_with_symbol_date(df_clean, targets=targets)\n",
        "\n",
        "# Feature extra\n",
        "df_processed_minmax[\"rsi_x_momentum\"] = df_processed_minmax[\"rsi_14\"] * df_processed_minmax[\"momentum_10\"]\n",
        "\n",
        "# Definir X e y\n",
        "X = df_processed_minmax.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "y = df_processed_minmax[\"target_3m\"]\n",
        "\n",
        "# VarianceThreshold\n",
        "selector = VarianceThreshold(threshold=1e-4)\n",
        "X_sel = pd.DataFrame(selector.fit_transform(X), index=X.index)\n",
        "features_selected = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "# División temporal\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Balanceo\n",
        "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "scale_weight = 0.9854333077829515  # reutilizamos valor óptimo\n",
        "\n",
        "# Entrenamiento\n",
        "model = XGBClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=scale_weight,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicción\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_prob > 0.4).astype(int)\n",
        "\n",
        "# Evaluación\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "# Guardar artefactos\n",
        "joblib.dump(model, \"model_xgb_3m.pkl\")\n",
        "joblib.dump(selector, \"selector_xgb_3m.pkl\")\n",
        "joblib.dump(features_selected, \"features_xgb_3m.pkl\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "XGBoost version: 2.1.4\nShape before: (10846547, 56)\nShape after: (10846547, 32)\ncolumnas scaler: Index(['symbol', 'close_v', 'volume', 'prev_close', 'prev_volume',\n       'daily_return', 'close_change_pct', 'intraday_volatility', 'log_return',\n       'volume_change_pct', 'sma_20', 'delta', 'gain', 'loss', 'rsi_14',\n       'rel_volume', 'ema_26', 'macd_signal', 'macd_histogram', 'true_range',\n       'atr_14', 'candle_body', 'upper_wick', 'lower_wick', 'candle_color',\n       'momentum_10', 'roc_10', 'var_95', 'target_6m', 'target_1y', 'year',\n       'month', 'dayofweek'],\n      dtype='object')\n              precision    recall  f1-score   support\n\n         0.0       0.73      0.53      0.61   1232988\n         1.0       0.54      0.74      0.63    936322\n\n    accuracy                           0.62   2169310\n   macro avg       0.63      0.63      0.62   2169310\nweighted avg       0.65      0.62      0.62   2169310\n\nROC AUC: 0.6910185207210477\nF1 Score: 0.6252430231329451\nConfusion Matrix:\n[[653615 579373]\n [246980 689342]]\nTN=653615, FP=579373, FN=246980, TP=689342\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "['features_xgb_3m.pkl']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "id": "_kqY4Ca7fhF-",
        "outputId": "a9d32039-b84d-4b29-fdb6-05844d9ce04e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score, confusion_matrix\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "\n",
        "print(\"XGBoost version:\", xgb.__version__)\n",
        "\n",
        "# Crear target binario con umbral del 5%\n",
        "df_full[\"target_1y\"] = (df_full[\"ret_next_1y\"] > 0.05).astype(int)\n",
        "\n",
        "# Ordenar y filtrar\n",
        "top_symbols = df_full[\"symbol\"].unique()\n",
        "df_clean = df_full[df_full[\"symbol\"].isin(top_symbols)].copy()\n",
        "df_clean = df_clean.sort_values(by=[\"symbol\", \"date\"])\n",
        "\n",
        "# Limpieza e imputación\n",
        "df_clean = clean_columns(df_clean)\n",
        "\n",
        "# Preprocesamiento\n",
        "targets = [\"target_1y\"]\n",
        "df_processed_minmax, _ = prepare_data_with_symbol_date(df_clean, targets=targets)\n",
        "\n",
        "# Feature extra\n",
        "df_processed_minmax[\"rsi_x_momentum\"] = df_processed_minmax[\"rsi_14\"] * df_processed_minmax[\"momentum_10\"]\n",
        "\n",
        "# Definir X e y\n",
        "X = df_processed_minmax.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "y = df_processed_minmax[\"target_1y\"]\n",
        "\n",
        "# VarianceThreshold\n",
        "selector = VarianceThreshold(threshold=1e-4)\n",
        "X_sel = pd.DataFrame(selector.fit_transform(X), index=X.index)\n",
        "features_selected = X.columns[selector.get_support()].tolist()\n",
        "\n",
        "# División temporal\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sel, y, shuffle=False, test_size=0.2)\n",
        "\n",
        "# Balanceo\n",
        "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
        "scale_weight = 0.9854333077829515  # mismo valor\n",
        "\n",
        "# Entrenamiento\n",
        "model = XGBClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    scale_pos_weight=scale_weight,\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predicción\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "y_pred = (y_prob > 0.4).astype(int)\n",
        "\n",
        "# Evaluación\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "# Guardar artefactos\n",
        "joblib.dump(model, \"model_xgb_1y.pkl\")\n",
        "joblib.dump(selector, \"selector_xgb_1y.pkl\")\n",
        "joblib.dump(features_selected, \"features_xgb_1y.pkl\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "XGBoost version: 2.1.4\nShape before: (10846547, 56)\nShape after: (10846547, 32)\ncolumnas scaler: Index(['symbol', 'close_v', 'volume', 'prev_close', 'prev_volume',\n       'daily_return', 'close_change_pct', 'intraday_volatility', 'log_return',\n       'volume_change_pct', 'sma_20', 'delta', 'gain', 'loss', 'rsi_14',\n       'rel_volume', 'ema_26', 'macd_signal', 'macd_histogram', 'true_range',\n       'atr_14', 'candle_body', 'upper_wick', 'lower_wick', 'candle_color',\n       'momentum_10', 'roc_10', 'var_95', 'target_3m', 'target_6m', 'year',\n       'month', 'dayofweek'],\n      dtype='object')\n              precision    recall  f1-score   support\n\n         0.0       0.74      0.36      0.49    971505\n         1.0       0.63      0.90      0.74   1197805\n\n    accuracy                           0.66   2169310\n   macro avg       0.69      0.63      0.61   2169310\nweighted avg       0.68      0.66      0.63   2169310\n\nROC AUC: 0.7295725889101499\nF1 Score: 0.742566203571656\nConfusion Matrix:\n[[ 352967  618538]\n [ 125180 1072625]]\nTN=352967, FP=618538, FN=125180, TP=1072625\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/plain": "['features_xgb_1y.pkl']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "id": "TppAEfzfgWc7",
        "outputId": "07f14d07-13d0-4618-e24d-8dbef3d197c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "es"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}