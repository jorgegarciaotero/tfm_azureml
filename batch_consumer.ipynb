{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install joblib adlfs scikit-learn keras-tuner\n",
        " \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: joblib in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.5.1)\nRequirement already satisfied: adlfs in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (2024.12.0)\nRequirement already satisfied: scikit-learn in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.7.0)\nRequirement already satisfied: keras-tuner in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (1.4.7)\nRequirement already satisfied: azure-core<2.0.0,>=1.28.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from adlfs) (1.33.0)\nRequirement already satisfied: azure-datalake-store<0.1,>=0.0.53 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from adlfs) (0.0.53)\nRequirement already satisfied: azure-identity in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from adlfs) (1.21.0)\nRequirement already satisfied: azure-storage-blob>=12.17.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from adlfs) (12.19.0)\nRequirement already satisfied: fsspec>=2023.12.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from adlfs) (2025.3.2)\nRequirement already satisfied: aiohttp>=3.7.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from adlfs) (3.12.13)\nRequirement already satisfied: numpy>=1.22.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-learn) (2.1.3)\nRequirement already satisfied: scipy>=1.8.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: keras in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from keras-tuner) (3.9.2)\nRequirement already satisfied: packaging in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from keras-tuner) (24.2)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from keras-tuner) (2.32.3)\nRequirement already satisfied: kt-legacy in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from keras-tuner) (1.0.5)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (6.5.0)\nRequirement already satisfied: propcache>=0.2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from aiohttp>=3.7.0->adlfs) (1.20.1)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.28.0->adlfs) (1.17.0)\nRequirement already satisfied: typing-extensions>=4.6.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.28.0->adlfs) (4.13.2)\nRequirement already satisfied: cffi in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from azure-datalake-store<0.1,>=0.0.53->adlfs) (1.17.1)\nRequirement already satisfied: msal<2,>=1.16.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from azure-datalake-store<0.1,>=0.0.53->adlfs) (1.32.0)\nRequirement already satisfied: cryptography>=2.1.4 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from azure-storage-blob>=12.17.0->adlfs) (44.0.2)\nRequirement already satisfied: isodate>=0.6.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from azure-storage-blob>=12.17.0->adlfs) (0.7.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->keras-tuner) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->keras-tuner) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->keras-tuner) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from requests->keras-tuner) (2025.1.31)\nRequirement already satisfied: msal-extensions>=1.2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from azure-identity->adlfs) (1.3.1)\nRequirement already satisfied: absl-py in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from keras->keras-tuner) (2.2.2)\nRequirement already satisfied: rich in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from keras->keras-tuner) (14.0.0)\nRequirement already satisfied: namex in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from keras->keras-tuner) (0.0.9)\nRequirement already satisfied: h5py in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from keras->keras-tuner) (3.13.0)\nRequirement already satisfied: optree in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from keras->keras-tuner) (0.15.0)\nRequirement already satisfied: ml-dtypes in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from keras->keras-tuner) (0.5.1)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from cffi->azure-datalake-store<0.1,>=0.0.53->adlfs) (2.22)\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2,>=1.16.0->azure-datalake-store<0.1,>=0.0.53->adlfs) (2.10.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from rich->keras->keras-tuner) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from rich->keras->keras-tuner) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1750778392047
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "import random\n",
        "import plotly.express as px\n",
        "import joblib\n",
        "from datetime import timedelta\n",
        "from typing import List, Tuple, Dict,Optional\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Azure\n",
        "from adlfs import AzureBlobFileSystem\n",
        "\n",
        "from typing import Tuple, List, Optional\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Models\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import  MinMaxScaler,LabelEncoder\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc ,  precision_score, recall_score\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "from keras_tuner import RandomSearch\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2025-06-24 15:18:14.840755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750778296.819141    4543 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750778297.333661    4543 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1750778302.517485    4543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1750778302.517510    4543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1750778302.517512    4543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1750778302.517514    4543 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-06-24 15:18:23.024187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "id": "LBwAr_3QsEcR",
        "gather": {
          "logged": 1750778325393
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FUNCTIONS"
      ],
      "metadata": {
        "id": "b1bBHtKRcHMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data_from_dl(account_name: str,container_name: str,relative_path: str,access_key: str)->pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads all Parquet files from an Azure Blob Storage path into a single DataFrame.\n",
        "    Args:\n",
        "        - account_name (str): Azure Storage account name.\n",
        "        - container_name (str): Name of the container.\n",
        "        - relative_path (str): Path inside the container to search for .parquet files.\n",
        "        - access_key (str): Storage account access key.\n",
        "    Returns:\n",
        "        - df (pd.DataFrame): Combined DataFrame from all found Parquet files.\n",
        "    Raises:\n",
        "        - ValueError: If no Parquet files are found in the path.\n",
        "    \"\"\"\n",
        "    abfs = AzureBlobFileSystem(account_name=account_name, account_key=access_key)\n",
        "\n",
        "\n",
        "    all_files = abfs.glob(f\"{container_name}/{relative_path}/*.parquet\")\n",
        "    print(f\"folder: {all_files}\")\n",
        "\n",
        "    if not all_files:\n",
        "        raise ValueError(\"Not found .parquet files\")\n",
        "\n",
        "    dfs = []\n",
        "    for f in all_files:\n",
        "        print(f\"Reading files: {f}\")\n",
        "        with abfs.open(f, \"rb\") as fp:\n",
        "            dfs.append(pd.read_parquet(fp))\n",
        "\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "    print(df.head())\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "ElZKSy8AdhJE",
        "gather": {
          "logged": 1750778325511
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(\n",
        "    df: pd.DataFrame,\n",
        "    targets: list\n",
        "):\n",
        "    \"\"\"\n",
        "    Prepares a DataFrame for LSTM modeling: imputes missing values and applies MinMax scaling.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input data with features + target + symbol + date.\n",
        "        targets (list): List of target column names.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Scaled dataframe with symbol and date preserved.\n",
        "        MinMaxScaler: The fitted scaler object.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Separar columnas a preservar\n",
        "    symbol_col = df_clean[\"symbol\"]\n",
        "    date_col = df_clean[\"date\"]\n",
        "\n",
        "    # Eliminar columnas no necesarias para el modelo\n",
        "    df_clean = df_clean.drop(columns=[\"symbol\", \"date\"], errors=\"ignore\")\n",
        "\n",
        "    # Codificación de categóricas si existieran (seguridad)\n",
        "    for col in df_clean.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
        "        df_clean[col] = LabelEncoder().fit_transform(df_clean[col].astype(str))\n",
        "\n",
        "    # Separar features y targets\n",
        "    feature_cols = [col for col in df_clean.columns if col not in targets]\n",
        "    X = df_clean[feature_cols]\n",
        "    y = df_clean[targets]\n",
        "\n",
        "    # Imputación + escalado\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "    df_scaled = pd.DataFrame(X_scaled, columns=feature_cols, index=df_clean.index)\n",
        "    df_scaled = pd.concat([df_scaled, y], axis=1)\n",
        "\n",
        "    # Reincorporar columnas para split posterior\n",
        "    df_scaled[\"symbol\"] = symbol_col.values\n",
        "    df_scaled[\"date\"] = date_col.values\n",
        "\n",
        "    return df_scaled, scaler\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "jM-z8jZdr_vg",
        "gather": {
          "logged": 1750778325616
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluation(\n",
        "    y_test: pd.Series,\n",
        "    y_pred: pd.Series,\n",
        "    y_prob: pd.Series\n",
        ") -> Tuple[float, float, float, np.ndarray, float, float]:\n",
        "    \"\"\"\n",
        "    Evaluates the classification model and plots metrics.\n",
        "\n",
        "    Args:\n",
        "        y_test (pd.Series): True target values.\n",
        "        y_pred (pd.Series): Predicted class values.\n",
        "        y_prob (pd.Series): Predicted probabilities for class 1.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing Accuracy, F1 Score, ROC AUC, Confusion Matrix, Precision, Recall.\n",
        "    \"\"\"\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    roc = roc_auc_score(y_test, y_prob)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"ROC AUC: {roc:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 4))\n",
        "    cax = ax.matshow(cm, cmap='Blues')\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticks([0, 1])\n",
        "    ax.set_yticks([0, 1])\n",
        "    ax.set_xticklabels(['No', 'Yes'])\n",
        "    ax.set_yticklabels(['No', 'Yes'])\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return acc, f1, roc, cm, precision, recall\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "mBcU6X6byprl",
        "gather": {
          "logged": 1750778325723
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Cleans a DataFrame by dropping unnecessary columns and handling missing values.\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): Input DataFrame.\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: Cleaned DataFrame.\n",
        "  \"\"\"\n",
        "  columns_to_drop = [\n",
        "      'capital_gains',\n",
        "      'ret_next_3m', 'ret_next_6m', 'ret_next_1y',\n",
        "      'price_lead_3m', 'price_lead_6m', 'price_lead_1y',\n",
        "      'open_v', 'high', 'low', 'dividends', 'stock_splits',\n",
        "      'is_dividend_day', 'is_stock_split', 'gap_open', 'price_range',\n",
        "      'tr_1', 'tr_2', 'tr_3', 'sma_5', 'bollinger_upper',\n",
        "      'bollinger_lower', 'ema_12', 'macd_line'\n",
        "  ]\n",
        "\n",
        "\n",
        "  print(f\"Shape before: {df.shape}\")\n",
        "  df = df.drop(columns=columns_to_drop, errors='ignore').copy()\n",
        "  numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\", \"int32\"]).columns\n",
        "  imputer = SimpleImputer(strategy=\"mean\")\n",
        "  df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
        "  print(f\"Shape after: {df.shape}\")\n",
        "  return df;"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "id": "K3119lEWi0ze",
        "gather": {
          "logged": 1750778325833
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_and_split_sequences_by_symbol(\n",
        "    df: pd.DataFrame,\n",
        "    target_column: str,\n",
        "    sequence_length: int = 60,\n",
        "    test_size: float = 0.2\n",
        "):\n",
        "    \"\"\"\n",
        "    Builds sequential data for each symbol independently and performs temporal train-test split.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Preprocessed DataFrame with features and target.\n",
        "        target_column (str): Column name for binary target.\n",
        "        sequence_length (int): Time window for each sequence.\n",
        "        test_size (float): Proportion of test samples (applied per symbol).\n",
        "\n",
        "    Returns:\n",
        "        Tuple of numpy arrays: X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    X_train, y_train, X_test, y_test = [], [], [], []\n",
        "\n",
        "    symbols = df[\"symbol\"].unique()\n",
        "    for symbol in symbols:\n",
        "        df_symbol = df[df[\"symbol\"] == symbol].copy()\n",
        "        df_symbol = df_symbol.sort_values(\"date\")\n",
        "\n",
        "        if len(df_symbol) <= sequence_length:\n",
        "            continue  # skip if not enough data\n",
        "\n",
        "        df_symbol[target_column] = df_symbol[target_column].astype(int)\n",
        "        features = df_symbol.drop(columns=[\"date\", \"symbol\", \"target_3m\", \"target_6m\", \"target_1y\"], errors=\"ignore\")\n",
        "        target = df_symbol[target_column].values\n",
        "\n",
        "        split_idx = int(len(features) * (1 - test_size))\n",
        "        for i in range(len(features) - sequence_length):\n",
        "            if i + sequence_length >= len(features):\n",
        "                continue  # avoid index error\n",
        "\n",
        "            X_seq = features.iloc[i:i + sequence_length].values.astype(np.float32)\n",
        "            y_val = target[i + sequence_length]\n",
        "\n",
        "            if i + sequence_length < split_idx:\n",
        "                X_train.append(X_seq)\n",
        "                y_train.append(y_val)\n",
        "            else:\n",
        "                X_test.append(X_seq)\n",
        "                y_test.append(y_val)\n",
        "\n",
        "    return (\n",
        "        np.array(X_train),\n",
        "        np.array(X_test),\n",
        "        np.array(y_train),\n",
        "        np.array(y_test)\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "cl3-fm7igBcr",
        "gather": {
          "logged": 1750778325935
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_dl(df,file_name,target_date):\n",
        "    full_path = f\"{container_name}/predictions/{file_name}/{file_name}_{target_date}.csv\"\n",
        "    print(full_path)\n",
        "    abfs = AzureBlobFileSystem(account_name=account_name, account_key=access_key)\n",
        "\n",
        "    # Write parquet\n",
        "    with abfs.open(full_path, \"wb\") as fp:\n",
        "        out_6m.to_csv(fp, index=False)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1750778326041
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_one_day(df_cut, model, scaler_minmax, target_column, seq_len, target_date):\n",
        "    \"\"\"\n",
        "    Realiza predicciones LSTM para cada símbolo con exactamente seq_len días previos a target_date.\n",
        "    \"\"\"\n",
        "    symbols = df_cut[\"symbol\"].unique()\n",
        "    df_last_seqs = []\n",
        "\n",
        "    for symbol in symbols:\n",
        "        df_sym = df_cut[df_cut[\"symbol\"] == symbol].copy().sort_values(\"date\")\n",
        "        if len(df_sym) >= seq_len:\n",
        "            df_seq = df_sym.tail(seq_len)\n",
        "            if len(df_seq) == seq_len:\n",
        "                df_last_seqs.append(df_seq)\n",
        "\n",
        "    df_pred = pd.concat(df_last_seqs).sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
        "    meta_cols = df_pred[[\"symbol\", \"date\"]].copy()\n",
        "\n",
        "    # Preprocesado como en entrenamiento\n",
        "    df_pred[\"date\"] = pd.to_datetime(df_pred[\"date\"], errors=\"coerce\")\n",
        "    df_pred[\"year\"] = df_pred[\"date\"].dt.year\n",
        "    df_pred[\"month\"] = df_pred[\"date\"].dt.month\n",
        "    df_pred[\"dayofweek\"] = df_pred[\"date\"].dt.dayofweek\n",
        "    df_pred[\"symbol\"] = LabelEncoder().fit_transform(df_pred[\"symbol\"].astype(str))\n",
        "\n",
        "    # Codificar categóricas si quedan\n",
        "    for col in df_pred.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
        "        df_pred[col] = LabelEncoder().fit_transform(df_pred[col].astype(str))\n",
        "\n",
        "    # Eliminar targets si están\n",
        "    for t in [\"target_3m\", \"target_6m\", \"target_1y\"]:\n",
        "        df_pred = df_pred.drop(columns=t, errors=\"ignore\")\n",
        "\n",
        "    # Selección exacta de columnas\n",
        "    feature_cols = [\n",
        "        'symbol','close_v', 'volume', 'prev_close', 'prev_volume', 'daily_return',\n",
        "        'close_change_pct', 'intraday_volatility', 'log_return', 'volume_change_pct',\n",
        "        'sma_20', 'delta', 'gain', 'loss', 'rsi_14', 'rel_volume',\n",
        "        'ema_26', 'macd_signal', 'macd_histogram', 'true_range', 'atr_14',\n",
        "        'candle_body', 'upper_wick', 'lower_wick', 'candle_color', 'momentum_10',\n",
        "        'roc_10', 'var_95', 'year', 'month', 'dayofweek'\n",
        "    ]\n",
        "    df_clean = df_pred[feature_cols]\n",
        "\n",
        "    # 🔽 Imputación\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    imputer = SimpleImputer(strategy=\"mean\")\n",
        "    df_clean[df_clean.columns] = imputer.fit_transform(df_clean)\n",
        "\n",
        "    # Escalado\n",
        "    X_scaled = scaler_minmax.transform(df_clean.values)\n",
        "    n_features = df_clean.shape[1]\n",
        "    n_symbols = X_scaled.shape[0] // seq_len\n",
        "    X_seq = X_scaled.astype(\"float32\").reshape(n_symbols, seq_len, n_features)\n",
        "\n",
        "    # Inferencia\n",
        "    y_prob = model.predict(X_seq, verbose=0).squeeze()\n",
        "    y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "    # Resultado final\n",
        "    symbols_out = (\n",
        "        meta_cols.groupby(np.arange(len(meta_cols)) // seq_len)\n",
        "        .tail(1)[\"symbol\"]\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"symbol\": symbols_out,\n",
        "        f\"p_up_{target_column}\": y_prob,\n",
        "        \"pred_up\": y_pred,\n",
        "        \"calc_date\": target_date\n",
        "    }).sort_values(f\"p_up_{target_column}\", ascending=False)\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1750778326133
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#MAIN**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"config.yaml\", \"r\") as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "account_name = config['storage']['storage_account_name']\n",
        "container_name = config['storage']['container_name']\n",
        "relative_path = config['storage']['relative_path']\n",
        "access_key = config['storage']['access_key']\n",
        "\n",
        "print(f\"{account_name}, {container_name}, {relative_path}, {access_key}\")\n",
        "# Cargar el DataFrame desde Azure\n",
        "df_full = load_data_from_dl(account_name, container_name, relative_path, access_key)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "smartwalletjorge, smart-wallet-dl, smart_wallet/stock_data_parquet, WVlHk0d7oGwjnRiJDICGexZY1LArsLiK7zMBw+GzYRWk9GCllLFAI0tizVY0ffrP9xobZoVdK8PP+AStXHGmfw==\nfolder: ['smart-wallet-dl/smart_wallet/stock_data_parquet/part-00000-tid-6319656906602864551-70988dce-8c71-437f-9993-81d0d24559f8-53-1-c000.snappy.parquet']\nReading files: smart-wallet-dl/smart_wallet/stock_data_parquet/part-00000-tid-6319656906602864551-70988dce-8c71-437f-9993-81d0d24559f8-53-1-c000.snappy.parquet\n         date  symbol      open_v        high         low     close_v  \\\n0  2023-07-25  TEL.OL   99.110617   99.423126   98.128441   98.128441   \n1  2023-07-25    ACIW   23.270000   23.530001   23.129999   23.420000   \n2  2023-07-25     TER  114.730270  117.172184  114.730270  116.368141   \n3  2023-07-25    ADCT    1.320000    1.330000    1.218000    1.290000   \n4  2023-07-25     TEX   59.353587   59.490459   57.574251   57.769783   \n\n      volume  dividends  stock_splits  capital_gains  ...  candle_color  \\\n0  1170518.0        0.0           0.0            NaN  ...           red   \n1   301500.0        0.0           0.0            NaN  ...         green   \n2  1256700.0        0.0           0.0            NaN  ...         green   \n3   811000.0        0.0           0.0            NaN  ...           red   \n4  1135700.0        0.0           0.0            NaN  ...           red   \n\n   momentum_10    roc_10    var_95  price_lead_3m  price_lead_6m  \\\n0     3.928711  0.041706 -0.016775     107.443146     113.383850   \n1     0.799999  0.035367 -0.031123      20.730000      30.410000   \n2     7.752586  0.071376 -0.020835      90.344109     110.100044   \n3    -0.560000 -0.302703 -0.068702       0.703000       2.100000   \n4    -1.759781 -0.029561 -0.032505      46.451183      56.749165   \n\n   price_lead_1y  ret_next_3m  ret_next_6m  ret_next_1y  \n0     125.240135     0.094924     0.155464     0.276288  \n1      42.599998    -0.114859     0.298463     0.818958  \n2     123.952568    -0.223635    -0.053864     0.065176  \n3       3.820000    -0.455039     0.627907     1.961240  \n4      65.014771    -0.195926    -0.017667     0.125411  \n\n[5 rows x 53 columns]\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "id": "MrJ33WDfD-kG",
        "gather": {
          "logged": 1750778367359
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargar modelo\n",
        "model_6m = load_model(f'pkls/improved_lstm.keras')\n",
        "scaler_minmax_6m = joblib.load(f'pkls/scaler_improved_lstm.pkl')\n",
        "targets = ['target_3m', 'target_6m', 'target_1y']"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "I0000 00:00:1750778372.944489    4543 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14793 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0001:00:00.0, compute capability: 7.5\n/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator MinMaxScaler from version 1.6.1 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1750778378774
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"max:\", df_full[\"date\"].max())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "max: 2025-05-16\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1750778379190
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_symbols = (\n",
        "    df_full.groupby(\"symbol\")\n",
        "    .size()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(500)\n",
        "    .index\n",
        ")\n",
        "df_full = df_full[df_full[\"symbol\"].isin(top_symbols)].copy()\n",
        "df_full[\"date\"]=pd.to_datetime(df_full[\"date\"])"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1750778381274
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_date = pd.Timestamp(\"2025-05-16\")\n",
        "seq_len = 60\n",
        "\n",
        "# Limitar al histórico disponible antes de la fecha objetivo\n",
        "df_filtered = df_full[df_full[\"date\"] < target_date].copy()\n",
        "\n",
        "# Asegurarse de que cada símbolo tiene datos hasta el día anterior\n",
        "latest_allowed = target_date - pd.Timedelta(days=1)\n",
        "valid_symbols = (\n",
        "    df_filtered[df_filtered[\"date\"] == latest_allowed][\"symbol\"]\n",
        "    .value_counts()\n",
        "    .loc[lambda x: x >= 1]\n",
        "    .index\n",
        ")\n",
        "\n",
        "# Filtrar y tomar solo los últimos `seq_len` registros de cada símbolo válido\n",
        "df_last = (\n",
        "    df_filtered[df_filtered[\"symbol\"].isin(valid_symbols)]\n",
        "    .sort_values([\"symbol\", \"date\"])\n",
        "    .groupby(\"symbol\", group_keys=False)\n",
        "    .tail(seq_len)\n",
        ")\n",
        "\n",
        "print(\"shape:\", df_last.shape)\n",
        "print(\"tipos:\", df_last.dtypes)\n",
        "print(\"min:\", df_last[\"date\"].min())\n",
        "print(\"max:\", df_last[\"date\"].max())\n",
        "print(f\"columns: {df_last.columns}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "shape: (30000, 53)\ntipos: date                   datetime64[ns]\nsymbol                         object\nopen_v                        float64\nhigh                          float64\nlow                           float64\nclose_v                       float64\nvolume                        float64\ndividends                     float64\nstock_splits                  float64\ncapital_gains                 float64\nprev_close                    float64\nprev_volume                   float64\ndaily_return                  float64\nclose_change_pct              float64\nintraday_volatility           float64\nprice_range                   float64\ngap_open                      float64\nlog_return                    float64\nvolume_change_pct             float64\nis_dividend_day                 int32\nis_stock_split                  int32\nsma_5                         float64\nsma_20                        float64\ndelta                         float64\ngain                          float64\nloss                          float64\nrsi_14                        float64\nbollinger_upper               float64\nbollinger_lower               float64\nrel_volume                    float64\nema_12                        float64\nema_26                        float64\nmacd_line                     float64\nmacd_signal                   float64\nmacd_histogram                float64\ntr_1                          float64\ntr_2                          float64\ntr_3                          float64\ntrue_range                    float64\natr_14                        float64\ncandle_body                   float64\nupper_wick                    float64\nlower_wick                    float64\ncandle_color                   object\nmomentum_10                   float64\nroc_10                        float64\nvar_95                        float64\nprice_lead_3m                 float64\nprice_lead_6m                 float64\nprice_lead_1y                 float64\nret_next_3m                   float64\nret_next_6m                   float64\nret_next_1y                   float64\ndtype: object\nmin: 2025-02-18 00:00:00\nmax: 2025-05-15 00:00:00\ncolumns: Index(['date', 'symbol', 'open_v', 'high', 'low', 'close_v', 'volume',\n       'dividends', 'stock_splits', 'capital_gains', 'prev_close',\n       'prev_volume', 'daily_return', 'close_change_pct',\n       'intraday_volatility', 'price_range', 'gap_open', 'log_return',\n       'volume_change_pct', 'is_dividend_day', 'is_stock_split', 'sma_5',\n       'sma_20', 'delta', 'gain', 'loss', 'rsi_14', 'bollinger_upper',\n       'bollinger_lower', 'rel_volume', 'ema_12', 'ema_26', 'macd_line',\n       'macd_signal', 'macd_histogram', 'tr_1', 'tr_2', 'tr_3', 'true_range',\n       'atr_14', 'candle_body', 'upper_wick', 'lower_wick', 'candle_color',\n       'momentum_10', 'roc_10', 'var_95', 'price_lead_3m', 'price_lead_6m',\n       'price_lead_1y', 'ret_next_3m', 'ret_next_6m', 'ret_next_1y'],\n      dtype='object')\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1750778383744
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_last.to_csv(\"df_last.csv\",sep=\";\")"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1750778386754
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_6m = predict_one_day(\n",
        "    df_last,\n",
        "    model_6m,\n",
        "    scaler_minmax_6m,\n",
        "    target_column=\"target_6m\",\n",
        "    seq_len=seq_len,\n",
        "    target_date=target_date\n",
        "    )\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_4543/268131502.py:47: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_clean[df_clean.columns] = imputer.fit_transform(df_clean)\nI0000 00:00:1750778388.667367    5315 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1750778390773
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(out_6m.head(20))\n",
        "out_6m.to_csv(\"out.csv\",sep=\";\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "        symbol  p_up_target_6m  pred_up  calc_date\n214       IBOC        0.619134        1 2025-05-16\n225    INGA.AS        0.618690        1 2025-05-16\n230     ISP.MI        0.615997        1 2025-05-16\n207   HUH1V.HE        0.615436        1 2025-05-16\n213     IBE.MC        0.614847        1 2025-05-16\n218        IEX        0.613991        1 2025-05-16\n181        GIC        0.613922        1 2025-05-16\n92         CMS        0.613264        1 2025-05-16\n209        HWC        0.613243        1 2025-05-16\n196       HDSN        0.613134        1 2025-05-16\n216       IDCC        0.612875        1 2025-05-16\n23         ALX        0.612789        1 2025-05-16\n195        HAS        0.612707        1 2025-05-16\n186       GOGL        0.612660        1 2025-05-16\n172  FORTUM.HE        0.612656        1 2025-05-16\n203        HRL        0.612597        1 2025-05-16\n219        IFF        0.612528        1 2025-05-16\n24       AM.PA        0.612327        1 2025-05-16\n194        HAL        0.611615        1 2025-05-16\n1      AALB.AS        0.611289        1 2025-05-16\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1750778390926
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_path = target_date.strftime(\"%Y%m%d\")  \n",
        "save_to_dl(out_6m,\"6_months\",target_path)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "smart-wallet-dl/predictions/6_months/6_months_20250516.csv\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1750778391029
        }
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.10 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "es"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}